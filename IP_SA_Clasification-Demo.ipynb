{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GiorgioMorales/HSI-BandSelection/blob/master/IP_SA_Clasification-Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSURcRgYvPAx"
   },
   "source": [
    "# TESTING BAND SELECTION METHODS FOR INDIAN PINES AND SALINAS HYPERSPECTRAL DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w77bIEaM76ab",
    "outputId": "fa52b45b-37af-444e-d0aa-161bf285348c",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:30:09.770354Z",
     "start_time": "2024-08-06T17:30:04.333252Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pn5kOLml9g5k"
   },
   "source": [
    "# Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TG1591mE9kXd",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:30:09.786311Z",
     "start_time": "2024-08-06T17:30:09.773346Z"
    }
   },
   "source": [
    "# Download Indian Pines dataset\n",
    "# !wget https://github.com/GiorgioMorales/HSI-BandSelection/raw/master/Data/Indian_pines_corrected.mat\n",
    "# !wget https://github.com/GiorgioMorales/HSI-BandSelection/raw/master/Data/Indian_pines_gt.mat\n",
    "#\n",
    "# # Download Salinas dataset\n",
    "# !wget https://github.com/GiorgioMorales/HSI-BandSelection/raw/master/Data/Salinas_corrected.mat\n",
    "# !wget https://github.com/GiorgioMorales/HSI-BandSelection/raw/master/Data/Salinas_gt.mat"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M4h1c5u-ZQ0"
   },
   "source": [
    "# Pre-process Datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IdpXkXDh-j8h",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:30:10.913665Z",
     "start_time": "2024-08-06T17:30:09.789305Z"
    }
   },
   "source": [
    "def loadata(name):\n",
    "    data_path = os.path.join(os.getcwd(), '')\n",
    "    if name == 'IP':\n",
    "        dat = sio.loadmat('Indian_pines_corrected.mat', verify_compressed_data_integrity=False)['indian_pines_corrected']\n",
    "        \n",
    "        label = sio.loadmat(os.path.join(data_path, 'Indian_pines_gt.mat'), verify_compressed_data_integrity=False)['indian_pines_gt']\n",
    "        return dat, label\n",
    "    elif name == 'SA':\n",
    "        dat = sio.loadmat(os.path.join(data_path, 'Salinas_corrected.mat'))['salinas_corrected']\n",
    "        label = sio.loadmat(os.path.join(data_path, 'Salinas_gt.mat'))['salinas_gt']\n",
    "        return dat, label\n",
    "\n",
    "\n",
    "def padWithZeros(Xc, margin=2):\n",
    "    newX = np.zeros((Xc.shape[0] + 2 * margin, Xc.shape[1] + 2 * margin, Xc.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:Xc.shape[0] + x_offset, y_offset:Xc.shape[1] + y_offset, :] = Xc\n",
    "    return newX\n",
    "\n",
    "\n",
    "def createImageCubes(Xc, yc, window=5, removeZeroLabels=True):\n",
    "    margin = int((window - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(Xc, margin=margin)\n",
    "    # split patches\n",
    "    patchesData = np.zeros((Xc.shape[0] * Xc.shape[1], window, window, Xc.shape[2]))\n",
    "    patchesLabels = np.zeros((Xc.shape[0] * Xc.shape[1]))\n",
    "    patchIndex = 0\n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = yc[r - margin, c - margin]\n",
    "            patchIndex = patchIndex + 1\n",
    "    if removeZeroLabels:\n",
    "        patchesData = patchesData[patchesLabels > 0, :, :, :]\n",
    "        patchesLabels = patchesLabels[patchesLabels > 0]\n",
    "        patchesLabels -= 1\n",
    "    return patchesData, patchesLabels\n",
    "\n",
    "# Load and pre-process the data\n",
    "data = 'IP'  # 'IP': Indian Pines, 'SA': Salinas \n",
    "trainx, train_y = loadata(data)\n",
    "trainx, train_y = createImageCubes(trainx, train_y, window=5)\n",
    "\n",
    "# Reshape as a 4-D TENSOR\n",
    "trainx = np.reshape(trainx, (trainx.shape[0], trainx.shape[1], trainx.shape[2],\n",
    "                             trainx.shape[3], 1))\n",
    "\n",
    "# Shuffle dataset and reduce dataset size\n",
    "np.random.seed(seed=7)  # Initialize seed to get reproducible results\n",
    "ind = [i for i in range(trainx.shape[0])]\n",
    "np.random.shuffle(ind)\n",
    "trainx = trainx[ind][:, :, :, :, :]\n",
    "train_y = train_y[ind][:]\n",
    "\n",
    "# Transpose dimensions to fit Pytorch order\n",
    "trainx = trainx.transpose((0, 4, 3, 1, 2))\n",
    "\n",
    "# Separate 50% of the dataset for training\n",
    "train_ind, val_ind = train_test_split(range(len(trainx)), test_size=0.50, random_state=7)\n",
    "trainX = np.array(trainx[train_ind])\n",
    "trainY = np.array(train_y[train_ind])\n",
    "print(trainX.shape)\n",
    "valX = np.array(trainx[val_ind])\n",
    "valY = np.array(train_y[val_ind])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5124, 1, 200, 5, 5)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAOeEt6uI9hv"
   },
   "source": [
    "# Define Classifier (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZAj89YvJE3P"
   },
   "source": [
    "In our paper, we used a CNN for our experiments: Hyper3DNet Lite: https://github.com/GiorgioMorales/HSI-BandSelection/blob/master/ClassificationStrategy/networks.py"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QLNHZyOsJD-_",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:30:10.944583Z",
     "start_time": "2024-08-06T17:30:10.915661Z"
    }
   },
   "source": [
    "from abc import ABC\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import reshape\n",
    "\n",
    "\n",
    "def weight_reset(m):\n",
    "    \"\"\"Reset model weights\"\"\"\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv3d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "\n",
    "class Hyper3DNetLite(nn.Module, ABC):\n",
    "    def __init__(self, img_shape=(1, 50, 25, 25), classes=2, data='Kochia'):\n",
    "        super(Hyper3DNetLite, self).__init__()\n",
    "        if data == 'Kochia' or data == 'Avocado':\n",
    "            stride = 2\n",
    "            out = 7\n",
    "        else:\n",
    "            stride = 1\n",
    "            out = 5\n",
    "        self.classes = classes\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        self.conv_layer1 = nn.Sequential(nn.Conv3d(in_channels=img_shape[0], out_channels=16, kernel_size=3, padding=1),\n",
    "                                         nn.ReLU(), nn.BatchNorm3d(16))\n",
    "        self.conv_layer2 = nn.Sequential(nn.Conv3d(in_channels=16, out_channels=16, kernel_size=3, padding=1),\n",
    "                                         nn.ReLU(), nn.BatchNorm3d(16))\n",
    "        self.sepconv1 = nn.Sequential(nn.Conv2d(in_channels=16 * img_shape[1], out_channels=16 * img_shape[1],\n",
    "                                                kernel_size=5, padding=2, groups=16 * img_shape[1]), nn.ReLU(),\n",
    "                                      nn.Conv2d(in_channels=16 * img_shape[1], out_channels=320,\n",
    "                                                kernel_size=1, padding=0), nn.ReLU(), nn.BatchNorm2d(320))\n",
    "        self.sepconv2 = nn.Sequential(nn.Conv2d(in_channels=320, out_channels=320,\n",
    "                                                kernel_size=3, padding=1, stride=stride, groups=320), nn.ReLU(),\n",
    "                                      nn.Conv2d(in_channels=320, out_channels=256,\n",
    "                                                kernel_size=1, padding=0), nn.ReLU(), nn.BatchNorm2d(256))\n",
    "        self.sepconv3 = nn.Sequential(nn.Conv2d(in_channels=256, out_channels=256,\n",
    "                                                kernel_size=3, padding=1, stride=stride, groups=256), nn.ReLU(),\n",
    "                                      nn.Conv2d(in_channels=256, out_channels=256,\n",
    "                                                kernel_size=1, padding=0), nn.ReLU(), nn.BatchNorm2d(256))\n",
    "        self.average = nn.AvgPool2d(kernel_size=out)\n",
    "\n",
    "        if classes == 2:\n",
    "            self.fc1 = nn.Linear(256, 1)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(256, self.classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 3D Feature extractor\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        # Reshape 3D-2D\n",
    "        x = reshape(x, (x.shape[0], self.img_shape[1] * 16, self.img_shape[2], self.img_shape[3]))\n",
    "        # 2D Spatial encoder\n",
    "        x = self.sepconv1(x)\n",
    "        x = self.sepconv2(x)\n",
    "        x = self.sepconv3(x)\n",
    "        # Global Average Pooling\n",
    "        x = self.average(x)\n",
    "        x = reshape(x, (x.shape[0], x.shape[1]))\n",
    "        if self.classes == 2:\n",
    "            x = self.fc1(x)\n",
    "        else:\n",
    "            x = self.fc1(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "prJgIKEAVH1H",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:30:10.991457Z",
     "start_time": "2024-08-06T17:30:10.946577Z"
    }
   },
   "source": [
    "import random\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "np.random.seed(seed=7)  # Initialize seed to get reproducible results\n",
    "random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "torch.cuda.manual_seed(7)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_class_distributionIP(train_y):\n",
    "    \"\"\"Get number of samples per class\"\"\"\n",
    "    count_dict = {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0, \"4\": 0, \"5\": 0, \"6\": 0, \"7\": 0, \"8\": 0, \"9\": 0, \"10\": 0, \"11\": 0,\n",
    "                  \"12\": 0, \"13\": 0, \"14\": 0, \"15\": 0}\n",
    "    for i in train_y:\n",
    "        if i == 0:\n",
    "            count_dict['0'] += 1\n",
    "        elif i == 1:\n",
    "            count_dict['1'] += 1\n",
    "        elif i == 2:\n",
    "            count_dict['2'] += 1\n",
    "        if i == 3:\n",
    "            count_dict['3'] += 1\n",
    "        elif i == 4:\n",
    "            count_dict['4'] += 1\n",
    "        elif i == 5:\n",
    "            count_dict['5'] += 1\n",
    "        if i == 6:\n",
    "            count_dict['6'] += 1\n",
    "        elif i == 7:\n",
    "            count_dict['7'] += 1\n",
    "        elif i == 8:\n",
    "            count_dict['8'] += 1\n",
    "        if i == 9:\n",
    "            count_dict['9'] += 1\n",
    "        elif i == 10:\n",
    "            count_dict['10'] += 1\n",
    "        elif i == 11:\n",
    "            count_dict['11'] += 1\n",
    "        if i == 12:\n",
    "            count_dict['12'] += 1\n",
    "        elif i == 13:\n",
    "            count_dict['13'] += 1\n",
    "        elif i == 14:\n",
    "            count_dict['14'] += 1\n",
    "        elif i == 15:\n",
    "            count_dict['15'] += 1\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "\n",
    "class CNNObject:\n",
    "    \"\"\"Helper class used to store the main information of a CNN for training\"\"\"\n",
    "\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.network = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "\n",
    "class CNNTrainer():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nbands = None\n",
    "        self.model = None\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.classes = None\n",
    "\n",
    "    def defineModel(self, nbands, windowSize, train_y):\n",
    "        \"\"\"Model declaration method\"\"\"\n",
    "        self.classes = len(np.unique(train_y))\n",
    "        model = Hyper3DNetLite(img_shape=(1, nbands, windowSize, windowSize), classes=int(self.classes), data=data)\n",
    "        model.to(self.device)\n",
    "        # Training parameters\n",
    "        class_count = [i for i in get_class_distributionIP(train_y).values()]\n",
    "        class_weights = 1. / torch.tensor(class_count, dtype=torch.float)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(self.device))\n",
    "\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "\n",
    "        self.nbands = nbands\n",
    "\n",
    "        self.model = CNNObject(model, criterion, optimizer)\n",
    "\n",
    "    def trainFold(self, trainx, trainy, batch_size, \n",
    "                          epochs, valx, valy, filepath, printProcess=False):\n",
    "        np.random.seed(seed=7)  # Initialize seed to get reproducible results (doesn't seem to work in Colab)\n",
    "        random.seed(7)\n",
    "        torch.manual_seed(7)\n",
    "        torch.cuda.manual_seed(7)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "      \n",
    "        print(\"Training model.....\")\n",
    "        # Prints summary of the modelif printProcess:      \n",
    "        if printProcess:\n",
    "            summary(self.model.network, (1, trainx.shape[2], trainx.shape[3], trainx.shape[4]))\n",
    "            \n",
    "        indexes = np.arange(len(trainx))  # Prepare list of indexes for shuffling\n",
    "        T = np.ceil(1.0 * len(trainx) / batch_size).astype(np.int32)  # Compute the number of steps in an epoch\n",
    "        val_acc = 0\n",
    "        loss = 1\n",
    "        for epoch in range(epochs):  # Epoch loop\n",
    "            # Shuffle indexes when epoch begins\n",
    "\n",
    "            self.model.network.train()  # Sets training mode\n",
    "            running_loss = 0.0\n",
    "            for step in range(T):  # Batch loop\n",
    "                # Generate indexes of the batch\n",
    "                inds = indexes[step * batch_size:(step + 1) * batch_size]\n",
    "\n",
    "                # Get actual batches\n",
    "                trainxb = torch.from_numpy(trainx[inds]).float().to(self.device)\n",
    "                trainyb = torch.from_numpy(trainy[inds]).long().to(self.device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                self.model.optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = self.model.network(trainxb)\n",
    "                loss = self.model.criterion(outputs, trainyb)\n",
    "                loss.backward()\n",
    "                self.model.optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if step % 10 == 9 and printProcess:  # print every 10 mini-batches\n",
    "                    print('[%d, %5d] loss: %.5f' %\n",
    "                          (epoch + 1, step + 1, running_loss / 10))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            # Validation step\n",
    "            ytest, ypred = self.evaluateFold(valx, valy, batch_size)\n",
    "            correct_pred = (np.array(ypred) == ytest).astype(float)\n",
    "            oa = correct_pred.sum() / len(correct_pred) * 100  # Calculate accuracy\n",
    "\n",
    "            # Save model if accuracy improves\n",
    "            if oa >= val_acc:\n",
    "                val_acc = oa\n",
    "                torch.save(self.model.network.state_dict(), filepath)  # saves checkpoint\n",
    "\n",
    "            if printProcess:\n",
    "                print('VALIDATION: Epoch %d, loss: %.5f, acc: %.3f, best_acc: %.3f' %\n",
    "                      (epoch + 1, loss.item(), oa.item(), val_acc))\n",
    "\n",
    "    def evaluateFold(self, valx, valy, batch_size):\n",
    "        ypred = []\n",
    "        with torch.no_grad():\n",
    "            self.model.network.eval()\n",
    "            Teva = np.ceil(1.0 * len(valx) / batch_size).astype(np.int32)\n",
    "            indtest = np.arange(len(valx))\n",
    "            for b in range(Teva):\n",
    "                inds = indtest[b * batch_size:(b + 1) * batch_size]\n",
    "                ypred_batch = self.model.network(torch.from_numpy(valx[inds]).float().to(self.device))\n",
    "                y_pred_softmax = torch.log_softmax(ypred_batch, dim=1)\n",
    "                _, y_pred_tags = torch.max(y_pred_softmax, dim=1)\n",
    "                ypred = ypred + (y_pred_tags.cpu().numpy()).tolist()\n",
    "        ytest = torch.from_numpy(valy).long().cpu().numpy()\n",
    "\n",
    "        return ytest, ypred\n",
    "\n",
    "    def loadModel(self, path):\n",
    "        # self.model.network.load_state_dict(torch.load(path))\n",
    "        self.model.network.load_state_dict(torch.load(path, weights_only=True))"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UsmTiDIKrW5"
   },
   "source": [
    "# Train and Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufysbKPbPWzW"
   },
   "source": [
    "Functions used to select a subset of bands and normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fEFJnLkiPV2_",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:30:11.006419Z",
     "start_time": "2024-08-06T17:30:10.995446Z"
    }
   },
   "source": [
    "def select(train_x, indexes):\n",
    "    temp = np.zeros((train_x.shape[0], 1, len(indexes), train_x.shape[3], train_x.shape[4]))\n",
    "    for nb in range(0, len(indexes)):\n",
    "        temp[:, :, nb, :, :] = train_x[:, :, indexes[nb], :, :]\n",
    "    train_x = temp.astype(np.float32)\n",
    "    return train_x\n",
    "\n",
    "def normalize(train_x):\n",
    "    \"\"\"Normalize and returns the calculated means and stds for each band\"\"\"\n",
    "    trainxn = train_x.copy()\n",
    "    dim = trainxn.shape[2]\n",
    "    means = np.zeros((dim, 1))\n",
    "    stds = np.zeros((dim, 1))\n",
    "    for n in range(dim): # Apply normalization to the data that is already in Pytorch format\n",
    "        means[n, ] = np.mean(trainxn[:, :, n, :, :])\n",
    "        stds[n, ] = np.std(trainxn[:, :, n, :, :])\n",
    "        trainxn[:, :, n, :, :] = (trainxn[:, :, n, :, :] - means[n, ]) / (stds[n, ])\n",
    "    return trainxn, means, stds\n",
    "\n",
    "\n",
    "def applynormalize(testx, means, stds):\n",
    "    \"\"\"Apply normalization based on previous calculated means and stds\"\"\"\n",
    "    testxn = testx.copy()\n",
    "    for n in range(testx.shape[2]):\n",
    "        testxn[:, :, n, :, :] = (testxn[:, :, n, :, :] - means[n, ]) / (stds[n, ])\n",
    "    return testxn"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-cXqMxfMB4G"
   },
   "source": [
    "Bands selected by our **Inter-Band Redundancy Analysis -- Greedy Spectral Selection (IBRA-GSS) method**: \n",
    "[11, 25, 34, 39, 67]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3exX4DQlJCnm",
    "outputId": "3489fcb0-0daf-4cde-82e0-6f2c33f7d9ee",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:30:36.809460Z",
     "start_time": "2024-08-06T17:30:11.009409Z"
    }
   },
   "source": [
    "indexes = [11, 25, 34, 39, 67]  # For SA: [37, 60, 82, 92, 175]\n",
    "train_X_selected = select(trainX, indexes)\n",
    "val_X_selected = select(valX, indexes)\n",
    "print(train_X_selected.shape)\n",
    "# Normalize using the training set\n",
    "train_X_selected, means, stds = normalize(train_X_selected)\n",
    "# Apply the same normalization to the validation set\n",
    "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
    "\n",
    "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
    "model = CNNTrainer()\n",
    "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
    "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
    "                        batch_size=128, epochs=50, filepath=\"temp_model\", printProcess=False)  # Set printProcess=True to see the training process \n",
    "\n",
    "# Validate\n",
    "model.loadModel(\"temp_model\")\n",
    "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
    "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
    "oa = correct_pred.sum() / len(correct_pred) * 100\n",
    "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
    "\n",
    "print(\"Accuracy = \" + str(oa))\n",
    "print(\"Precision = \" + str(prec))\n",
    "print(\"Recall = \" + str(rec))\n",
    "print(\"F1 = \" + str(f1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5124, 1, 5, 5, 5)\n",
      "Training model.....\n",
      "Accuracy = 97.95121951219512\n",
      "Precision = 0.977346683668659\n",
      "Recall = 0.9771219214840877\n",
      "F1 = 0.9771105143370269\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jjyBQO5Qw7Ji",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:30:36.840350Z",
     "start_time": "2024-08-06T17:30:36.811426Z"
    }
   },
   "source": [
    "# Reset weights\n",
    "model.model.network.apply(weight_reset)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hyper3DNetLite(\n",
       "  (conv_layer1): Sequential(\n",
       "    (0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_layer2): Sequential(\n",
       "    (0): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sepconv1): Sequential(\n",
       "    (0): Conv2d(80, 80, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=80)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sepconv2): Sequential(\n",
       "    (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sepconv3): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (average): AvgPool2d(kernel_size=5, stride=5, padding=0)\n",
       "  (fc1): Linear(in_features=256, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "To2DP0dTxZah"
   },
   "source": [
    "Bands selected by the Fast Neighborhood Grouping Method for Hyperspectral Band Selection (FNGBS) method (https://github.com/qianngli/FNGBS): [28, 70, 92, 107, 129]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xVWT4Up1xyhi",
    "outputId": "91094151-31e5-4b19-8c01-527b859b3593",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:31:00.909952Z",
     "start_time": "2024-08-06T17:30:36.842343Z"
    }
   },
   "source": [
    "indexes = [28, 70, 92, 107, 129]  # For SA: [16, 31, 113, 132, 175]\n",
    "train_X_selected = select(trainX, indexes)\n",
    "val_X_selected = select(valX, indexes)\n",
    "\n",
    "# Normalize using the training set\n",
    "train_X_selected, means, stds = normalize(train_X_selected)\n",
    "# Apply the same normalization to the validation set\n",
    "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
    "\n",
    "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
    "model = CNNTrainer()\n",
    "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
    "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
    "                        batch_size=128, epochs=50, filepath=\"temp_model\")\n",
    "\n",
    "# Validate\n",
    "model.loadModel(\"temp_model\")\n",
    "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
    "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
    "oa = correct_pred.sum() / len(correct_pred) * 100\n",
    "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
    "\n",
    "print(\"Accuracy = \" + str(oa))\n",
    "print(\"Precision = \" + str(prec))\n",
    "print(\"Recall = \" + str(rec))\n",
    "print(\"F1 = \" + str(f1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.....\n",
      "Accuracy = 97.4829268292683\n",
      "Precision = 0.9579502628892422\n",
      "Recall = 0.977417704864443\n",
      "F1 = 0.9670382062199967\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yZZ4SukKz31o",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:31:00.925909Z",
     "start_time": "2024-08-06T17:31:00.911948Z"
    }
   },
   "source": [
    "# Reset weights\n",
    "model.model.network.apply(weight_reset)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hyper3DNetLite(\n",
       "  (conv_layer1): Sequential(\n",
       "    (0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_layer2): Sequential(\n",
       "    (0): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sepconv1): Sequential(\n",
       "    (0): Conv2d(80, 80, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=80)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sepconv2): Sequential(\n",
       "    (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sepconv3): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (average): AvgPool2d(kernel_size=5, stride=5, padding=0)\n",
       "  (fc1): Linear(in_features=256, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZXJ1e2kzHqI"
   },
   "source": [
    "Bands selected by the Similarity-Based Ranking (SRSSIM) method (https://ieeexplore.ieee.org/document/9324974): [28, 52, 91, 104, 121]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MgeqBSsvzd_4",
    "outputId": "b1d29a51-be77-408f-ef42-3f2a76427301",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:31:25.184192Z",
     "start_time": "2024-08-06T17:31:00.927904Z"
    }
   },
   "source": [
    "indexes = [28, 52, 91, 104, 121]  # For SA: [5, 47, 61, 81, 201]\n",
    "train_X_selected = select(trainX, indexes)\n",
    "val_X_selected = select(valX, indexes)\n",
    "\n",
    "# Normalize using the training set\n",
    "train_X_selected, means, stds = normalize(train_X_selected)\n",
    "# Apply the same normalization to the validation set\n",
    "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
    "\n",
    "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
    "model = CNNTrainer()\n",
    "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
    "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
    "                        batch_size=128, epochs=50, filepath=\"temp_model\")\n",
    "\n",
    "# Validate\n",
    "model.loadModel(\"temp_model\")\n",
    "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
    "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
    "oa = correct_pred.sum() / len(correct_pred) * 100\n",
    "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
    "\n",
    "print(\"Accuracy = \" + str(oa))\n",
    "print(\"Precision = \" + str(prec))\n",
    "print(\"Recall = \" + str(rec))\n",
    "print(\"F1 = \" + str(f1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.....\n",
      "Accuracy = 97.30731707317074\n",
      "Precision = 0.9777133518057587\n",
      "Recall = 0.9613271969900963\n",
      "F1 = 0.9673736168458024\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5HoxzQdLz4zY",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:31:25.200149Z",
     "start_time": "2024-08-06T17:31:25.185189Z"
    }
   },
   "source": [
    "# Reset weights\n",
    "model.model.network.apply(weight_reset)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hyper3DNetLite(\n",
       "  (conv_layer1): Sequential(\n",
       "    (0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_layer2): Sequential(\n",
       "    (0): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sepconv1): Sequential(\n",
       "    (0): Conv2d(80, 80, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=80)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sepconv2): Sequential(\n",
       "    (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sepconv3): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (average): AvgPool2d(kernel_size=5, stride=5, padding=0)\n",
       "  (fc1): Linear(in_features=256, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykOuh7qgzkjP"
   },
   "source": [
    "Bands selected by the Optimal Clustering Framework (OCF) method (https://ieeexplore.ieee.org/document/8356741/): [16, 28, 50, 67, 90]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3a8XJuzzzEY",
    "outputId": "cd1e43cb-b2ad-43c6-d2cf-347362f49974",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:31:49.435853Z",
     "start_time": "2024-08-06T17:31:25.202145Z"
    }
   },
   "source": [
    "indexes = [16, 28, 50, 67, 90]  # For SA: [34, 45, 58, 93, 120]\n",
    "train_X_selected = select(trainX, indexes)\n",
    "val_X_selected = select(valX, indexes)\n",
    "\n",
    "# Normalize using the training set\n",
    "train_X_selected, means, stds = normalize(train_X_selected)\n",
    "# Apply the same normalization to the validation set\n",
    "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
    "\n",
    "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
    "model = CNNTrainer()\n",
    "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
    "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
    "                        batch_size=128, epochs=50, filepath=\"temp_model\")\n",
    "\n",
    "# Validate\n",
    "model.loadModel(\"temp_model\")\n",
    "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
    "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
    "oa = correct_pred.sum() / len(correct_pred) * 100\n",
    "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
    "\n",
    "print(\"Accuracy = \" + str(oa))\n",
    "print(\"Precision = \" + str(prec))\n",
    "print(\"Recall = \" + str(rec))\n",
    "print(\"F1 = \" + str(f1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.....\n",
      "Accuracy = 96.74146341463414\n",
      "Precision = 0.9641367379340147\n",
      "Recall = 0.956995159654451\n",
      "F1 = 0.959212474256282\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GG63KQSCz5nG",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:31:49.450813Z",
     "start_time": "2024-08-06T17:31:49.437847Z"
    }
   },
   "source": [
    "# Reset weights\n",
    "model.model.network.apply(weight_reset)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hyper3DNetLite(\n",
       "  (conv_layer1): Sequential(\n",
       "    (0): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_layer2): Sequential(\n",
       "    (0): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sepconv1): Sequential(\n",
       "    (0): Conv2d(80, 80, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=80)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(80, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sepconv2): Sequential(\n",
       "    (0): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (sepconv3): Sequential(\n",
       "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (average): AvgPool2d(kernel_size=5, stride=5, padding=0)\n",
       "  (fc1): Linear(in_features=256, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdkXpb4u0IPx"
   },
   "source": [
    "Bands selected by the Histogram Assisted Genetic Algorithm for Reduction in Dimensionality (HAGRID) method (https://www.researchgate.net/publication/334216691_Using_a_genetic_algorithm_with_histogram-based_feature_selection_in_hyperspectral_image_classification): [17, 31, 55, 75, 119]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46vkNNJI0jeB",
    "outputId": "83eb9da7-692d-40ed-bc92-08c5467aa501",
    "ExecuteTime": {
     "end_time": "2024-08-06T17:32:13.910810Z",
     "start_time": "2024-08-06T17:31:49.452808Z"
    }
   },
   "source": [
    "indexes = [17, 31, 55, 75, 119]  # For SA: [13, 20, 31, 44, 84]\n",
    "train_X_selected = select(trainX, indexes)\n",
    "val_X_selected = select(valX, indexes)\n",
    "\n",
    "# Normalize using the training set\n",
    "train_X_selected, means, stds = normalize(train_X_selected)\n",
    "# Apply the same normalization to the validation set\n",
    "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
    "\n",
    "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
    "model = CNNTrainer()\n",
    "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
    "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
    "                        batch_size=128, epochs=50, filepath=\"temp_model\")\n",
    "\n",
    "# Validate\n",
    "model.loadModel(\"temp_model\")\n",
    "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
    "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
    "oa = correct_pred.sum() / len(correct_pred) * 100\n",
    "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
    "\n",
    "print(\"Accuracy = \" + str(oa))\n",
    "print(\"Precision = \" + str(prec))\n",
    "print(\"Recall = \" + str(rec))\n",
    "print(\"F1 = \" + str(f1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.....\n",
      "Accuracy = 96.56585365853658\n",
      "Precision = 0.9707242033096664\n",
      "Recall = 0.9464380188176214\n",
      "F1 = 0.9567226855037398\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T17:32:13.925770Z",
     "start_time": "2024-08-06T17:32:13.911808Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": 15
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM2+1uh4qaJcHK+hQ7TZMpg",
   "include_colab_link": true,
   "name": "IP-SA-Demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
