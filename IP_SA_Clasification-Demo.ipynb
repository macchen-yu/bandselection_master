{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "IP-SA-Demo.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM2+1uh4qaJcHK+hQ7TZMpg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiorgioMorales/HSI-BandSelection/blob/master/IP_SA_Clasification-Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSURcRgYvPAx"
      },
      "source": [
        "# TESTING BAND SELECTION METHODS FOR INDIAN PINES AND SALINAS HYPERSPECTRAL DATASETS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w77bIEaM76ab",
        "outputId": "fa52b45b-37af-444e-d0aa-161bf285348c"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn5kOLml9g5k"
      },
      "source": [
        "# Download Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG1591mE9kXd"
      },
      "source": [
        "# Download Indian Pines dataset\n",
        "!wget https://github.com/GiorgioMorales/HSI-BandSelection/raw/master/Data/Indian_pines_corrected.mat\n",
        "!wget https://github.com/GiorgioMorales/HSI-BandSelection/raw/master/Data/Indian_pines_gt.mat\n",
        "\n",
        "# Download Salinas dataset\n",
        "!wget https://github.com/GiorgioMorales/HSI-BandSelection/raw/master/Data/Salinas_corrected.mat\n",
        "!wget https://github.com/GiorgioMorales/HSI-BandSelection/raw/master/Data/Salinas_gt.mat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M4h1c5u-ZQ0"
      },
      "source": [
        "# Pre-process Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdpXkXDh-j8h"
      },
      "source": [
        "def loadata(name):\n",
        "    data_path = os.path.join(os.getcwd(), '')\n",
        "    if name == 'IP':\n",
        "        dat = sio.loadmat('Indian_pines_corrected.mat', verify_compressed_data_integrity=False)['indian_pines_corrected']\n",
        "        label = sio.loadmat(os.path.join(data_path, 'Indian_pines_gt.mat'), verify_compressed_data_integrity=False)['indian_pines_gt']\n",
        "        return dat, label\n",
        "    elif name == 'SA':\n",
        "        dat = sio.loadmat(os.path.join(data_path, 'Salinas_corrected.mat'))['salinas_corrected']\n",
        "        label = sio.loadmat(os.path.join(data_path, 'Salinas_gt.mat'))['salinas_gt']\n",
        "        return dat, label\n",
        "\n",
        "\n",
        "def padWithZeros(Xc, margin=2):\n",
        "    newX = np.zeros((Xc.shape[0] + 2 * margin, Xc.shape[1] + 2 * margin, Xc.shape[2]))\n",
        "    x_offset = margin\n",
        "    y_offset = margin\n",
        "    newX[x_offset:Xc.shape[0] + x_offset, y_offset:Xc.shape[1] + y_offset, :] = Xc\n",
        "    return newX\n",
        "\n",
        "\n",
        "def createImageCubes(Xc, yc, window=5, removeZeroLabels=True):\n",
        "    margin = int((window - 1) / 2)\n",
        "    zeroPaddedX = padWithZeros(Xc, margin=margin)\n",
        "    # split patches\n",
        "    patchesData = np.zeros((Xc.shape[0] * Xc.shape[1], window, window, Xc.shape[2]))\n",
        "    patchesLabels = np.zeros((Xc.shape[0] * Xc.shape[1]))\n",
        "    patchIndex = 0\n",
        "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
        "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
        "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
        "            patchesData[patchIndex, :, :, :] = patch\n",
        "            patchesLabels[patchIndex] = yc[r - margin, c - margin]\n",
        "            patchIndex = patchIndex + 1\n",
        "    if removeZeroLabels:\n",
        "        patchesData = patchesData[patchesLabels > 0, :, :, :]\n",
        "        patchesLabels = patchesLabels[patchesLabels > 0]\n",
        "        patchesLabels -= 1\n",
        "    return patchesData, patchesLabels\n",
        "\n",
        "# Load and pre-process the data\n",
        "data = 'IP'  # 'IP': Indian Pines, 'SA': Salinas \n",
        "trainx, train_y = loadata(data)\n",
        "trainx, train_y = createImageCubes(trainx, train_y, window=5)\n",
        "\n",
        "# Reshape as a 4-D TENSOR\n",
        "trainx = np.reshape(trainx, (trainx.shape[0], trainx.shape[1], trainx.shape[2],\n",
        "                             trainx.shape[3], 1))\n",
        "\n",
        "# Shuffle dataset and reduce dataset size\n",
        "np.random.seed(seed=7)  # Initialize seed to get reproducible results\n",
        "ind = [i for i in range(trainx.shape[0])]\n",
        "np.random.shuffle(ind)\n",
        "trainx = trainx[ind][:, :, :, :, :]\n",
        "train_y = train_y[ind][:]\n",
        "\n",
        "# Transpose dimensions to fit Pytorch order\n",
        "trainx = trainx.transpose((0, 4, 3, 1, 2))\n",
        "\n",
        "# Separate 50% of the dataset for training\n",
        "train_ind, val_ind = train_test_split(range(len(trainx)), test_size=0.50, random_state=7)\n",
        "trainX = np.array(trainx[train_ind])\n",
        "trainY = np.array(train_y[train_ind])\n",
        "valX = np.array(trainx[val_ind])\n",
        "valY = np.array(train_y[val_ind])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAOeEt6uI9hv"
      },
      "source": [
        "# Define Classifier (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZAj89YvJE3P"
      },
      "source": [
        "In our paper, we used a CNN for our experiments: Hyper3DNet Lite: https://github.com/GiorgioMorales/HSI-BandSelection/blob/master/ClassificationStrategy/networks.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLNHZyOsJD-_"
      },
      "source": [
        "from abc import ABC\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch import reshape\n",
        "\n",
        "\n",
        "def weight_reset(m):\n",
        "    \"\"\"Reset model weights\"\"\"\n",
        "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv3d) or isinstance(m, nn.Linear):\n",
        "        m.reset_parameters()\n",
        "\n",
        "\n",
        "class Hyper3DNetLite(nn.Module, ABC):\n",
        "    def __init__(self, img_shape=(1, 50, 25, 25), classes=2, data='Kochia'):\n",
        "        super(Hyper3DNetLite, self).__init__()\n",
        "        if data == 'Kochia' or data == 'Avocado':\n",
        "            stride = 2\n",
        "            out = 7\n",
        "        else:\n",
        "            stride = 1\n",
        "            out = 5\n",
        "        self.classes = classes\n",
        "        self.img_shape = img_shape\n",
        "\n",
        "        self.conv_layer1 = nn.Sequential(nn.Conv3d(in_channels=img_shape[0], out_channels=16, kernel_size=3, padding=1),\n",
        "                                         nn.ReLU(), nn.BatchNorm3d(16))\n",
        "        self.conv_layer2 = nn.Sequential(nn.Conv3d(in_channels=16, out_channels=16, kernel_size=3, padding=1),\n",
        "                                         nn.ReLU(), nn.BatchNorm3d(16))\n",
        "        self.sepconv1 = nn.Sequential(nn.Conv2d(in_channels=16 * img_shape[1], out_channels=16 * img_shape[1],\n",
        "                                                kernel_size=5, padding=2, groups=16 * img_shape[1]), nn.ReLU(),\n",
        "                                      nn.Conv2d(in_channels=16 * img_shape[1], out_channels=320,\n",
        "                                                kernel_size=1, padding=0), nn.ReLU(), nn.BatchNorm2d(320))\n",
        "        self.sepconv2 = nn.Sequential(nn.Conv2d(in_channels=320, out_channels=320,\n",
        "                                                kernel_size=3, padding=1, stride=stride, groups=320), nn.ReLU(),\n",
        "                                      nn.Conv2d(in_channels=320, out_channels=256,\n",
        "                                                kernel_size=1, padding=0), nn.ReLU(), nn.BatchNorm2d(256))\n",
        "        self.sepconv3 = nn.Sequential(nn.Conv2d(in_channels=256, out_channels=256,\n",
        "                                                kernel_size=3, padding=1, stride=stride, groups=256), nn.ReLU(),\n",
        "                                      nn.Conv2d(in_channels=256, out_channels=256,\n",
        "                                                kernel_size=1, padding=0), nn.ReLU(), nn.BatchNorm2d(256))\n",
        "        self.average = nn.AvgPool2d(kernel_size=out)\n",
        "\n",
        "        if classes == 2:\n",
        "            self.fc1 = nn.Linear(256, 1)\n",
        "        else:\n",
        "            self.fc1 = nn.Linear(256, self.classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 3D Feature extractor\n",
        "        x = self.conv_layer1(x)\n",
        "        x = self.conv_layer2(x)\n",
        "        # Reshape 3D-2D\n",
        "        x = reshape(x, (x.shape[0], self.img_shape[1] * 16, self.img_shape[2], self.img_shape[3]))\n",
        "        # 2D Spatial encoder\n",
        "        x = self.sepconv1(x)\n",
        "        x = self.sepconv2(x)\n",
        "        x = self.sepconv3(x)\n",
        "        # Global Average Pooling\n",
        "        x = self.average(x)\n",
        "        x = reshape(x, (x.shape[0], x.shape[1]))\n",
        "        if self.classes == 2:\n",
        "            x = self.fc1(x)\n",
        "        else:\n",
        "            x = self.fc1(x)\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prJgIKEAVH1H"
      },
      "source": [
        "import random\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "np.random.seed(seed=7)  # Initialize seed to get reproducible results\n",
        "random.seed(7)\n",
        "torch.manual_seed(7)\n",
        "torch.cuda.manual_seed(7)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def get_class_distributionIP(train_y):\n",
        "    \"\"\"Get number of samples per class\"\"\"\n",
        "    count_dict = {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0, \"4\": 0, \"5\": 0, \"6\": 0, \"7\": 0, \"8\": 0, \"9\": 0, \"10\": 0, \"11\": 0,\n",
        "                  \"12\": 0, \"13\": 0, \"14\": 0, \"15\": 0}\n",
        "    for i in train_y:\n",
        "        if i == 0:\n",
        "            count_dict['0'] += 1\n",
        "        elif i == 1:\n",
        "            count_dict['1'] += 1\n",
        "        elif i == 2:\n",
        "            count_dict['2'] += 1\n",
        "        if i == 3:\n",
        "            count_dict['3'] += 1\n",
        "        elif i == 4:\n",
        "            count_dict['4'] += 1\n",
        "        elif i == 5:\n",
        "            count_dict['5'] += 1\n",
        "        if i == 6:\n",
        "            count_dict['6'] += 1\n",
        "        elif i == 7:\n",
        "            count_dict['7'] += 1\n",
        "        elif i == 8:\n",
        "            count_dict['8'] += 1\n",
        "        if i == 9:\n",
        "            count_dict['9'] += 1\n",
        "        elif i == 10:\n",
        "            count_dict['10'] += 1\n",
        "        elif i == 11:\n",
        "            count_dict['11'] += 1\n",
        "        if i == 12:\n",
        "            count_dict['12'] += 1\n",
        "        elif i == 13:\n",
        "            count_dict['13'] += 1\n",
        "        elif i == 14:\n",
        "            count_dict['14'] += 1\n",
        "        elif i == 15:\n",
        "            count_dict['15'] += 1\n",
        "\n",
        "    return count_dict\n",
        "\n",
        "\n",
        "class CNNObject:\n",
        "    \"\"\"Helper class used to store the main information of a CNN for training\"\"\"\n",
        "\n",
        "    def __init__(self, model, criterion, optimizer):\n",
        "        self.network = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "\n",
        "class CNNTrainer():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.nbands = None\n",
        "        self.model = None\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.classes = None\n",
        "\n",
        "    def defineModel(self, nbands, windowSize, train_y):\n",
        "        \"\"\"Model declaration method\"\"\"\n",
        "        self.classes = len(np.unique(train_y))\n",
        "        model = Hyper3DNetLite(img_shape=(1, nbands, windowSize, windowSize), classes=int(self.classes), data=data)\n",
        "        model.to(self.device)\n",
        "        # Training parameters\n",
        "        class_count = [i for i in get_class_distributionIP(train_y).values()]\n",
        "        class_weights = 1. / torch.tensor(class_count, dtype=torch.float)\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(self.device))\n",
        "\n",
        "        optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
        "\n",
        "        self.nbands = nbands\n",
        "\n",
        "        self.model = CNNObject(model, criterion, optimizer)\n",
        "\n",
        "    def trainFold(self, trainx, trainy, batch_size, \n",
        "                          epochs, valx, valy, filepath, printProcess=False):\n",
        "        np.random.seed(seed=7)  # Initialize seed to get reproducible results (doesn't seem to work in Colab)\n",
        "        random.seed(7)\n",
        "        torch.manual_seed(7)\n",
        "        torch.cuda.manual_seed(7)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "      \n",
        "        print(\"Training model.....\")\n",
        "        # Prints summary of the modelif printProcess:      \n",
        "        if printProcess:\n",
        "            summary(self.model.network, (1, trainx.shape[2], trainx.shape[3], trainx.shape[4]))\n",
        "            \n",
        "        indexes = np.arange(len(trainx))  # Prepare list of indexes for shuffling\n",
        "        T = np.ceil(1.0 * len(trainx) / batch_size).astype(np.int32)  # Compute the number of steps in an epoch\n",
        "        val_acc = 0\n",
        "        loss = 1\n",
        "        for epoch in range(epochs):  # Epoch loop\n",
        "            # Shuffle indexes when epoch begins\n",
        "\n",
        "            self.model.network.train()  # Sets training mode\n",
        "            running_loss = 0.0\n",
        "            for step in range(T):  # Batch loop\n",
        "                # Generate indexes of the batch\n",
        "                inds = indexes[step * batch_size:(step + 1) * batch_size]\n",
        "\n",
        "                # Get actual batches\n",
        "                trainxb = torch.from_numpy(trainx[inds]).float().to(self.device)\n",
        "                trainyb = torch.from_numpy(trainy[inds]).long().to(self.device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                self.model.optimizer.zero_grad()\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs = self.model.network(trainxb)\n",
        "                loss = self.model.criterion(outputs, trainyb)\n",
        "                loss.backward()\n",
        "                self.model.optimizer.step()\n",
        "\n",
        "                # print statistics\n",
        "                running_loss += loss.item()\n",
        "                if step % 10 == 9 and printProcess:  # print every 10 mini-batches\n",
        "                    print('[%d, %5d] loss: %.5f' %\n",
        "                          (epoch + 1, step + 1, running_loss / 10))\n",
        "                    running_loss = 0.0\n",
        "\n",
        "            # Validation step\n",
        "            ytest, ypred = self.evaluateFold(valx, valy, batch_size)\n",
        "            correct_pred = (np.array(ypred) == ytest).astype(float)\n",
        "            oa = correct_pred.sum() / len(correct_pred) * 100  # Calculate accuracy\n",
        "\n",
        "            # Save model if accuracy improves\n",
        "            if oa >= val_acc:\n",
        "                val_acc = oa\n",
        "                torch.save(self.model.network.state_dict(), filepath)  # saves checkpoint\n",
        "\n",
        "            if printProcess:\n",
        "                print('VALIDATION: Epoch %d, loss: %.5f, acc: %.3f, best_acc: %.3f' %\n",
        "                      (epoch + 1, loss.item(), oa.item(), val_acc))\n",
        "\n",
        "    def evaluateFold(self, valx, valy, batch_size):\n",
        "        ypred = []\n",
        "        with torch.no_grad():\n",
        "            self.model.network.eval()\n",
        "            Teva = np.ceil(1.0 * len(valx) / batch_size).astype(np.int32)\n",
        "            indtest = np.arange(len(valx))\n",
        "            for b in range(Teva):\n",
        "                inds = indtest[b * batch_size:(b + 1) * batch_size]\n",
        "                ypred_batch = self.model.network(torch.from_numpy(valx[inds]).float().to(self.device))\n",
        "                y_pred_softmax = torch.log_softmax(ypred_batch, dim=1)\n",
        "                _, y_pred_tags = torch.max(y_pred_softmax, dim=1)\n",
        "                ypred = ypred + (y_pred_tags.cpu().numpy()).tolist()\n",
        "        ytest = torch.from_numpy(valy).long().cpu().numpy()\n",
        "\n",
        "        return ytest, ypred\n",
        "\n",
        "    def loadModel(self, path):\n",
        "        self.model.network.load_state_dict(torch.load(path))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UsmTiDIKrW5"
      },
      "source": [
        "# Train and Compare Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufysbKPbPWzW"
      },
      "source": [
        "Functions used to select a subset of bands and normalize the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEFJnLkiPV2_"
      },
      "source": [
        "def select(train_x, indexes):\n",
        "    temp = np.zeros((train_x.shape[0], 1, len(indexes), train_x.shape[3], train_x.shape[4]))\n",
        "    for nb in range(0, len(indexes)):\n",
        "        temp[:, :, nb, :, :] = train_x[:, :, indexes[nb], :, :]\n",
        "    train_x = temp.astype(np.float32)\n",
        "    return train_x\n",
        "\n",
        "def normalize(train_x):\n",
        "    \"\"\"Normalize and returns the calculated means and stds for each band\"\"\"\n",
        "    trainxn = train_x.copy()\n",
        "    dim = trainxn.shape[2]\n",
        "    means = np.zeros((dim, 1))\n",
        "    stds = np.zeros((dim, 1))\n",
        "    for n in range(dim): # Apply normalization to the data that is already in Pytorch format\n",
        "        means[n, ] = np.mean(trainxn[:, :, n, :, :])\n",
        "        stds[n, ] = np.std(trainxn[:, :, n, :, :])\n",
        "        trainxn[:, :, n, :, :] = (trainxn[:, :, n, :, :] - means[n, ]) / (stds[n, ])\n",
        "    return trainxn, means, stds\n",
        "\n",
        "\n",
        "def applynormalize(testx, means, stds):\n",
        "    \"\"\"Apply normalization based on previous calculated means and stds\"\"\"\n",
        "    testxn = testx.copy()\n",
        "    for n in range(testx.shape[2]):\n",
        "        testxn[:, :, n, :, :] = (testxn[:, :, n, :, :] - means[n, ]) / (stds[n, ])\n",
        "    return testxn"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-cXqMxfMB4G"
      },
      "source": [
        "Bands selected by our **Inter-Band Redundancy Analysis -- Greedy Spectral Selection (IBRA-GSS) method**: \n",
        "[11, 25, 34, 39, 67]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3exX4DQlJCnm",
        "outputId": "3489fcb0-0daf-4cde-82e0-6f2c33f7d9ee"
      },
      "source": [
        "indexes = [11, 25, 34, 39, 67]  # For SA: [37, 60, 82, 92, 175]\n",
        "train_X_selected = select(trainX, indexes)\n",
        "val_X_selected = select(valX, indexes)\n",
        "\n",
        "# Normalize using the training set\n",
        "train_X_selected, means, stds = normalize(train_X_selected)\n",
        "# Apply the same normalization to the validation set\n",
        "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
        "\n",
        "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
        "model = CNNTrainer()\n",
        "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
        "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
        "                        batch_size=128, epochs=50, filepath=\"temp_model\", printProcess=False)  # Set printProcess=True to see the training process \n",
        "\n",
        "# Validate\n",
        "model.loadModel(\"temp_model\")\n",
        "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
        "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
        "oa = correct_pred.sum() / len(correct_pred) * 100\n",
        "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
        "\n",
        "print(\"Accuracy = \" + str(oa))\n",
        "print(\"Precision = \" + str(prec))\n",
        "print(\"Recall = \" + str(rec))\n",
        "print(\"F1 = \" + str(f1))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model.....\n",
            "Accuracy = 98.18536585365854\n",
            "Precision = 0.9833378649246366\n",
            "Recall = 0.9805571561269105\n",
            "F1 = 0.9817216848978283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjyBQO5Qw7Ji"
      },
      "source": [
        "# Reset weights\n",
        "model.model.network.apply(weight_reset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To2DP0dTxZah"
      },
      "source": [
        "Bands selected by the Fast Neighborhood Grouping Method for Hyperspectral Band Selection (FNGBS) method (https://github.com/qianngli/FNGBS): [28, 70, 92, 107, 129]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVWT4Up1xyhi",
        "outputId": "91094151-31e5-4b19-8c01-527b859b3593"
      },
      "source": [
        "indexes = [28, 70, 92, 107, 129]  # For SA: [16, 31, 113, 132, 175]\n",
        "train_X_selected = select(trainX, indexes)\n",
        "val_X_selected = select(valX, indexes)\n",
        "\n",
        "# Normalize using the training set\n",
        "train_X_selected, means, stds = normalize(train_X_selected)\n",
        "# Apply the same normalization to the validation set\n",
        "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
        "\n",
        "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
        "model = CNNTrainer()\n",
        "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
        "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
        "                        batch_size=128, epochs=50, filepath=\"temp_model\")\n",
        "\n",
        "# Validate\n",
        "model.loadModel(\"temp_model\")\n",
        "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
        "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
        "oa = correct_pred.sum() / len(correct_pred) * 100\n",
        "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
        "\n",
        "print(\"Accuracy = \" + str(oa))\n",
        "print(\"Precision = \" + str(prec))\n",
        "print(\"Recall = \" + str(rec))\n",
        "print(\"F1 = \" + str(f1))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model.....\n",
            "Accuracy = 96.9560975609756\n",
            "Precision = 0.9486142406432961\n",
            "Recall = 0.9633240193810757\n",
            "F1 = 0.9549830786866147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZZ4SukKz31o"
      },
      "source": [
        "# Reset weights\n",
        "model.model.network.apply(weight_reset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZXJ1e2kzHqI"
      },
      "source": [
        "Bands selected by the Similarity-Based Ranking (SRSSIM) method (https://ieeexplore.ieee.org/document/9324974): [28, 52, 91, 104, 121]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgeqBSsvzd_4",
        "outputId": "b1d29a51-be77-408f-ef42-3f2a76427301"
      },
      "source": [
        "indexes = [28, 52, 91, 104, 121]  # For SA: [5, 47, 61, 81, 201]\n",
        "train_X_selected = select(trainX, indexes)\n",
        "val_X_selected = select(valX, indexes)\n",
        "\n",
        "# Normalize using the training set\n",
        "train_X_selected, means, stds = normalize(train_X_selected)\n",
        "# Apply the same normalization to the validation set\n",
        "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
        "\n",
        "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
        "model = CNNTrainer()\n",
        "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
        "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
        "                        batch_size=128, epochs=50, filepath=\"temp_model\")\n",
        "\n",
        "# Validate\n",
        "model.loadModel(\"temp_model\")\n",
        "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
        "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
        "oa = correct_pred.sum() / len(correct_pred) * 100\n",
        "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
        "\n",
        "print(\"Accuracy = \" + str(oa))\n",
        "print(\"Precision = \" + str(prec))\n",
        "print(\"Recall = \" + str(rec))\n",
        "print(\"F1 = \" + str(f1))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model.....\n",
            "Accuracy = 97.26829268292683\n",
            "Precision = 0.9606307317052267\n",
            "Recall = 0.9572251907795821\n",
            "F1 = 0.9585275998455447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HoxzQdLz4zY"
      },
      "source": [
        "# Reset weights\n",
        "model.model.network.apply(weight_reset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykOuh7qgzkjP"
      },
      "source": [
        "Bands selected by the Optimal Clustering Framework (OCF) method (https://ieeexplore.ieee.org/document/8356741/): [16, 28, 50, 67, 90]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3a8XJuzzzEY",
        "outputId": "cd1e43cb-b2ad-43c6-d2cf-347362f49974"
      },
      "source": [
        "indexes = [16, 28, 50, 67, 90]  # For SA: [34, 45, 58, 93, 120]\n",
        "train_X_selected = select(trainX, indexes)\n",
        "val_X_selected = select(valX, indexes)\n",
        "\n",
        "# Normalize using the training set\n",
        "train_X_selected, means, stds = normalize(train_X_selected)\n",
        "# Apply the same normalization to the validation set\n",
        "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
        "\n",
        "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
        "model = CNNTrainer()\n",
        "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
        "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
        "                        batch_size=128, epochs=50, filepath=\"temp_model\")\n",
        "\n",
        "# Validate\n",
        "model.loadModel(\"temp_model\")\n",
        "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
        "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
        "oa = correct_pred.sum() / len(correct_pred) * 100\n",
        "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
        "\n",
        "print(\"Accuracy = \" + str(oa))\n",
        "print(\"Precision = \" + str(prec))\n",
        "print(\"Recall = \" + str(rec))\n",
        "print(\"F1 = \" + str(f1))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model.....\n",
            "Accuracy = 96.58536585365853\n",
            "Precision = 0.9605385147207325\n",
            "Recall = 0.9617955690283178\n",
            "F1 = 0.9607409243482415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GG63KQSCz5nG"
      },
      "source": [
        "# Reset weights\n",
        "model.model.network.apply(weight_reset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdkXpb4u0IPx"
      },
      "source": [
        "Bands selected by the Histogram Assisted Genetic Algorithm for Reduction in Dimensionality (HAGRID) method (https://www.researchgate.net/publication/334216691_Using_a_genetic_algorithm_with_histogram-based_feature_selection_in_hyperspectral_image_classification): [17, 31, 55, 75, 119]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46vkNNJI0jeB",
        "outputId": "83eb9da7-692d-40ed-bc92-08c5467aa501"
      },
      "source": [
        "indexes = [17, 31, 55, 75, 119]  # For SA: [13, 20, 31, 44, 84]\n",
        "train_X_selected = select(trainX, indexes)\n",
        "val_X_selected = select(valX, indexes)\n",
        "\n",
        "# Normalize using the training set\n",
        "train_X_selected, means, stds = normalize(train_X_selected)\n",
        "# Apply the same normalization to the validation set\n",
        "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
        "\n",
        "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
        "model = CNNTrainer()\n",
        "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
        "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
        "                        batch_size=128, epochs=50, filepath=\"temp_model\")\n",
        "\n",
        "# Validate\n",
        "model.loadModel(\"temp_model\")\n",
        "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
        "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
        "oa = correct_pred.sum() / len(correct_pred) * 100\n",
        "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
        "\n",
        "print(\"Accuracy = \" + str(oa))\n",
        "print(\"Precision = \" + str(prec))\n",
        "print(\"Recall = \" + str(rec))\n",
        "print(\"F1 = \" + str(f1))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model.....\n",
            "Accuracy = 96.46829268292683\n",
            "Precision = 0.9612188059575583\n",
            "Recall = 0.9577183658032549\n",
            "F1 = 0.9585461002239718\n"
          ]
        }
      ]
    }
  ]
}