{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GiorgioMorales/HSI-BandSelection/blob/master/IP_SA_Clasification-Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSURcRgYvPAx"
   },
   "source": [
    "# TESTING BAND SELECTION METHODS FOR INDIAN PINES AND SALINAS HYPERSPECTRAL DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w77bIEaM76ab",
    "outputId": "fa52b45b-37af-444e-d0aa-161bf285348c",
    "ExecuteTime": {
     "end_time": "2024-08-06T08:02:37.778587Z",
     "start_time": "2024-08-06T08:02:34.822128Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pn5kOLml9g5k"
   },
   "source": [
    "# Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T06:45:47.391458Z",
     "start_time": "2024-08-06T06:45:47.390462Z"
    },
    "id": "TG1591mE9kXd"
   },
   "outputs": [],
   "source": [
    "# Download Indian Pines dataset\n",
    "# !wget https://github.com/GiorgioMorales/HSI-BandSelection/raw/master/Data/Indian_pines_corrected.mat\n",
    "# !wget https://github.com/GiorgioMorales/HSI-BandSelection/raw/master/Data/Indian_pines_gt.mat\n",
    "#\n",
    "# # Download Salinas dataset\n",
    "# !wget https://github.com/GiorgioMorales/HSI-BandSelection/raw/master/Data/Salinas_corrected.mat\n",
    "# !wget https://github.com/GiorgioMorales/HSI-BandSelection/raw/master/Data/Salinas_gt.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M4h1c5u-ZQ0"
   },
   "source": [
    "# Pre-process Datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IdpXkXDh-j8h",
    "ExecuteTime": {
     "end_time": "2024-08-06T08:07:51.776134Z",
     "start_time": "2024-08-06T08:07:50.544856Z"
    }
   },
   "source": [
    "def loadata(name):\n",
    "    data_path = os.path.join(os.getcwd(), '')\n",
    "    if name == 'IP':\n",
    "        dat = sio.loadmat('Indian_pines_corrected.mat', verify_compressed_data_integrity=False)['indian_pines_corrected']\n",
    "        \n",
    "        label = sio.loadmat(os.path.join(data_path, 'Indian_pines_gt.mat'), verify_compressed_data_integrity=False)['indian_pines_gt']\n",
    "        return dat, label\n",
    "    elif name == 'SA':\n",
    "        dat = sio.loadmat(os.path.join(data_path, 'Salinas_corrected.mat'))['salinas_corrected']\n",
    "        label = sio.loadmat(os.path.join(data_path, 'Salinas_gt.mat'))['salinas_gt']\n",
    "        return dat, label\n",
    "\n",
    "\n",
    "def padWithZeros(Xc, margin=2):\n",
    "    newX = np.zeros((Xc.shape[0] + 2 * margin, Xc.shape[1] + 2 * margin, Xc.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:Xc.shape[0] + x_offset, y_offset:Xc.shape[1] + y_offset, :] = Xc\n",
    "    return newX\n",
    "\n",
    "\n",
    "def createImageCubes(Xc, yc, window=5, removeZeroLabels=True):\n",
    "    margin = int((window - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(Xc, margin=margin)\n",
    "    # split patches\n",
    "    patchesData = np.zeros((Xc.shape[0] * Xc.shape[1], window, window, Xc.shape[2]))\n",
    "    patchesLabels = np.zeros((Xc.shape[0] * Xc.shape[1]))\n",
    "    patchIndex = 0\n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = yc[r - margin, c - margin]\n",
    "            patchIndex = patchIndex + 1\n",
    "    if removeZeroLabels:\n",
    "        patchesData = patchesData[patchesLabels > 0, :, :, :]\n",
    "        patchesLabels = patchesLabels[patchesLabels > 0]\n",
    "        patchesLabels -= 1\n",
    "    return patchesData, patchesLabels\n",
    "\n",
    "# Load and pre-process the data\n",
    "data = 'IP'  # 'IP': Indian Pines, 'SA': Salinas \n",
    "trainx, train_y = loadata(data)\n",
    "trainx, train_y = createImageCubes(trainx, train_y, window=5)\n",
    "\n",
    "# Reshape as a 4-D TENSOR\n",
    "trainx = np.reshape(trainx, (trainx.shape[0], trainx.shape[1], trainx.shape[2],\n",
    "                             trainx.shape[3], 1))\n",
    "\n",
    "# Shuffle dataset and reduce dataset size\n",
    "np.random.seed(seed=7)  # Initialize seed to get reproducible results\n",
    "ind = [i for i in range(trainx.shape[0])]\n",
    "np.random.shuffle(ind)\n",
    "trainx = trainx[ind][:, :, :, :, :]\n",
    "train_y = train_y[ind][:]\n",
    "\n",
    "# Transpose dimensions to fit Pytorch order\n",
    "trainx = trainx.transpose((0, 4, 3, 1, 2))\n",
    "\n",
    "# Separate 50% of the dataset for training\n",
    "train_ind, val_ind = train_test_split(range(len(trainx)), test_size=0.50, random_state=7)\n",
    "trainX = np.array(trainx[train_ind])\n",
    "trainY = np.array(train_y[train_ind])\n",
    "print(trainX.shape)\n",
    "valX = np.array(trainx[val_ind])\n",
    "valY = np.array(train_y[val_ind])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5124, 1, 200, 5, 5)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAOeEt6uI9hv"
   },
   "source": [
    "# Define Classifier (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZAj89YvJE3P"
   },
   "source": [
    "In our paper, we used a CNN for our experiments: Hyper3DNet Lite: https://github.com/GiorgioMorales/HSI-BandSelection/blob/master/ClassificationStrategy/networks.py"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QLNHZyOsJD-_",
    "ExecuteTime": {
     "end_time": "2024-08-06T08:07:56.952252Z",
     "start_time": "2024-08-06T08:07:56.924326Z"
    }
   },
   "source": [
    "from abc import ABC\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import reshape\n",
    "\n",
    "\n",
    "def weight_reset(m):\n",
    "    \"\"\"Reset model weights\"\"\"\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Conv3d) or isinstance(m, nn.Linear):\n",
    "        m.reset_parameters()\n",
    "\n",
    "\n",
    "class Hyper3DNetLite(nn.Module, ABC):\n",
    "    def __init__(self, img_shape=(1, 50, 25, 25), classes=2, data='Kochia'):\n",
    "        super(Hyper3DNetLite, self).__init__()\n",
    "        if data == 'Kochia' or data == 'Avocado':\n",
    "            stride = 2\n",
    "            out = 7\n",
    "        else:\n",
    "            stride = 1\n",
    "            out = 5\n",
    "        self.classes = classes\n",
    "        self.img_shape = img_shape\n",
    "\n",
    "        self.conv_layer1 = nn.Sequential(nn.Conv3d(in_channels=img_shape[0], out_channels=16, kernel_size=3, padding=1),\n",
    "                                         nn.ReLU(), nn.BatchNorm3d(16))\n",
    "        self.conv_layer2 = nn.Sequential(nn.Conv3d(in_channels=16, out_channels=16, kernel_size=3, padding=1),\n",
    "                                         nn.ReLU(), nn.BatchNorm3d(16))\n",
    "        self.sepconv1 = nn.Sequential(nn.Conv2d(in_channels=16 * img_shape[1], out_channels=16 * img_shape[1],\n",
    "                                                kernel_size=5, padding=2, groups=16 * img_shape[1]), nn.ReLU(),\n",
    "                                      nn.Conv2d(in_channels=16 * img_shape[1], out_channels=320,\n",
    "                                                kernel_size=1, padding=0), nn.ReLU(), nn.BatchNorm2d(320))\n",
    "        self.sepconv2 = nn.Sequential(nn.Conv2d(in_channels=320, out_channels=320,\n",
    "                                                kernel_size=3, padding=1, stride=stride, groups=320), nn.ReLU(),\n",
    "                                      nn.Conv2d(in_channels=320, out_channels=256,\n",
    "                                                kernel_size=1, padding=0), nn.ReLU(), nn.BatchNorm2d(256))\n",
    "        self.sepconv3 = nn.Sequential(nn.Conv2d(in_channels=256, out_channels=256,\n",
    "                                                kernel_size=3, padding=1, stride=stride, groups=256), nn.ReLU(),\n",
    "                                      nn.Conv2d(in_channels=256, out_channels=256,\n",
    "                                                kernel_size=1, padding=0), nn.ReLU(), nn.BatchNorm2d(256))\n",
    "        self.average = nn.AvgPool2d(kernel_size=out)\n",
    "\n",
    "        if classes == 2:\n",
    "            self.fc1 = nn.Linear(256, 1)\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(256, self.classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # 3D Feature extractor\n",
    "        x = self.conv_layer1(x)\n",
    "        x = self.conv_layer2(x)\n",
    "        # Reshape 3D-2D\n",
    "        x = reshape(x, (x.shape[0], self.img_shape[1] * 16, self.img_shape[2], self.img_shape[3]))\n",
    "        # 2D Spatial encoder\n",
    "        x = self.sepconv1(x)\n",
    "        x = self.sepconv2(x)\n",
    "        x = self.sepconv3(x)\n",
    "        # Global Average Pooling\n",
    "        x = self.average(x)\n",
    "        x = reshape(x, (x.shape[0], x.shape[1]))\n",
    "        if self.classes == 2:\n",
    "            x = self.fc1(x)\n",
    "        else:\n",
    "            x = self.fc1(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "prJgIKEAVH1H",
    "ExecuteTime": {
     "end_time": "2024-08-06T08:08:00.346454Z",
     "start_time": "2024-08-06T08:08:00.301574Z"
    }
   },
   "source": [
    "import random\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "np.random.seed(seed=7)  # Initialize seed to get reproducible results\n",
    "random.seed(7)\n",
    "torch.manual_seed(7)\n",
    "torch.cuda.manual_seed(7)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def get_class_distributionIP(train_y):\n",
    "    \"\"\"Get number of samples per class\"\"\"\n",
    "    count_dict = {\"0\": 0, \"1\": 0, \"2\": 0, \"3\": 0, \"4\": 0, \"5\": 0, \"6\": 0, \"7\": 0, \"8\": 0, \"9\": 0, \"10\": 0, \"11\": 0,\n",
    "                  \"12\": 0, \"13\": 0, \"14\": 0, \"15\": 0}\n",
    "    for i in train_y:\n",
    "        if i == 0:\n",
    "            count_dict['0'] += 1\n",
    "        elif i == 1:\n",
    "            count_dict['1'] += 1\n",
    "        elif i == 2:\n",
    "            count_dict['2'] += 1\n",
    "        if i == 3:\n",
    "            count_dict['3'] += 1\n",
    "        elif i == 4:\n",
    "            count_dict['4'] += 1\n",
    "        elif i == 5:\n",
    "            count_dict['5'] += 1\n",
    "        if i == 6:\n",
    "            count_dict['6'] += 1\n",
    "        elif i == 7:\n",
    "            count_dict['7'] += 1\n",
    "        elif i == 8:\n",
    "            count_dict['8'] += 1\n",
    "        if i == 9:\n",
    "            count_dict['9'] += 1\n",
    "        elif i == 10:\n",
    "            count_dict['10'] += 1\n",
    "        elif i == 11:\n",
    "            count_dict['11'] += 1\n",
    "        if i == 12:\n",
    "            count_dict['12'] += 1\n",
    "        elif i == 13:\n",
    "            count_dict['13'] += 1\n",
    "        elif i == 14:\n",
    "            count_dict['14'] += 1\n",
    "        elif i == 15:\n",
    "            count_dict['15'] += 1\n",
    "\n",
    "    return count_dict\n",
    "\n",
    "\n",
    "class CNNObject:\n",
    "    \"\"\"Helper class used to store the main information of a CNN for training\"\"\"\n",
    "\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        self.network = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "\n",
    "class CNNTrainer():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nbands = None\n",
    "        self.model = None\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.classes = None\n",
    "\n",
    "    def defineModel(self, nbands, windowSize, train_y):\n",
    "        \"\"\"Model declaration method\"\"\"\n",
    "        self.classes = len(np.unique(train_y))\n",
    "        model = Hyper3DNetLite(img_shape=(1, nbands, windowSize, windowSize), classes=int(self.classes), data=data)\n",
    "        model.to(self.device)\n",
    "        # Training parameters\n",
    "        class_count = [i for i in get_class_distributionIP(train_y).values()]\n",
    "        class_weights = 1. / torch.tensor(class_count, dtype=torch.float)\n",
    "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(self.device))\n",
    "\n",
    "        optimizer = optim.Adadelta(model.parameters(), lr=1.0)\n",
    "\n",
    "        self.nbands = nbands\n",
    "\n",
    "        self.model = CNNObject(model, criterion, optimizer)\n",
    "\n",
    "    def trainFold(self, trainx, trainy, batch_size, \n",
    "                          epochs, valx, valy, filepath, printProcess=False):\n",
    "        np.random.seed(seed=7)  # Initialize seed to get reproducible results (doesn't seem to work in Colab)\n",
    "        random.seed(7)\n",
    "        torch.manual_seed(7)\n",
    "        torch.cuda.manual_seed(7)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "      \n",
    "        print(\"Training model.....\")\n",
    "        # Prints summary of the modelif printProcess:      \n",
    "        if printProcess:\n",
    "            summary(self.model.network, (1, trainx.shape[2], trainx.shape[3], trainx.shape[4]))\n",
    "            \n",
    "        indexes = np.arange(len(trainx))  # Prepare list of indexes for shuffling\n",
    "        T = np.ceil(1.0 * len(trainx) / batch_size).astype(np.int32)  # Compute the number of steps in an epoch\n",
    "        val_acc = 0\n",
    "        loss = 1\n",
    "        for epoch in range(epochs):  # Epoch loop\n",
    "            # Shuffle indexes when epoch begins\n",
    "\n",
    "            self.model.network.train()  # Sets training mode\n",
    "            running_loss = 0.0\n",
    "            for step in range(T):  # Batch loop\n",
    "                # Generate indexes of the batch\n",
    "                inds = indexes[step * batch_size:(step + 1) * batch_size]\n",
    "\n",
    "                # Get actual batches\n",
    "                trainxb = torch.from_numpy(trainx[inds]).float().to(self.device)\n",
    "                trainyb = torch.from_numpy(trainy[inds]).long().to(self.device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                self.model.optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = self.model.network(trainxb)\n",
    "                loss = self.model.criterion(outputs, trainyb)\n",
    "                loss.backward()\n",
    "                self.model.optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "                if step % 10 == 9 and printProcess:  # print every 10 mini-batches\n",
    "                    print('[%d, %5d] loss: %.5f' %\n",
    "                          (epoch + 1, step + 1, running_loss / 10))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "            # Validation step\n",
    "            ytest, ypred = self.evaluateFold(valx, valy, batch_size)\n",
    "            correct_pred = (np.array(ypred) == ytest).astype(float)\n",
    "            oa = correct_pred.sum() / len(correct_pred) * 100  # Calculate accuracy\n",
    "\n",
    "            # Save model if accuracy improves\n",
    "            if oa >= val_acc:\n",
    "                val_acc = oa\n",
    "                torch.save(self.model.network.state_dict(), filepath)  # saves checkpoint\n",
    "\n",
    "            if printProcess:\n",
    "                print('VALIDATION: Epoch %d, loss: %.5f, acc: %.3f, best_acc: %.3f' %\n",
    "                      (epoch + 1, loss.item(), oa.item(), val_acc))\n",
    "\n",
    "    def evaluateFold(self, valx, valy, batch_size):\n",
    "        ypred = []\n",
    "        with torch.no_grad():\n",
    "            self.model.network.eval()\n",
    "            Teva = np.ceil(1.0 * len(valx) / batch_size).astype(np.int32)\n",
    "            indtest = np.arange(len(valx))\n",
    "            for b in range(Teva):\n",
    "                inds = indtest[b * batch_size:(b + 1) * batch_size]\n",
    "                ypred_batch = self.model.network(torch.from_numpy(valx[inds]).float().to(self.device))\n",
    "                y_pred_softmax = torch.log_softmax(ypred_batch, dim=1)\n",
    "                _, y_pred_tags = torch.max(y_pred_softmax, dim=1)\n",
    "                ypred = ypred + (y_pred_tags.cpu().numpy()).tolist()\n",
    "        ytest = torch.from_numpy(valy).long().cpu().numpy()\n",
    "\n",
    "        return ytest, ypred\n",
    "\n",
    "    def loadModel(self, path):\n",
    "        self.model.network.load_state_dict(torch.load(path))"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UsmTiDIKrW5"
   },
   "source": [
    "# Train and Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufysbKPbPWzW"
   },
   "source": [
    "Functions used to select a subset of bands and normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fEFJnLkiPV2_",
    "ExecuteTime": {
     "end_time": "2024-08-06T08:08:02.660619Z",
     "start_time": "2024-08-06T08:08:02.646656Z"
    }
   },
   "source": [
    "def select(train_x, indexes):\n",
    "    temp = np.zeros((train_x.shape[0], 1, len(indexes), train_x.shape[3], train_x.shape[4]))\n",
    "    for nb in range(0, len(indexes)):\n",
    "        temp[:, :, nb, :, :] = train_x[:, :, indexes[nb], :, :]\n",
    "    train_x = temp.astype(np.float32)\n",
    "    return train_x\n",
    "\n",
    "def normalize(train_x):\n",
    "    \"\"\"Normalize and returns the calculated means and stds for each band\"\"\"\n",
    "    trainxn = train_x.copy()\n",
    "    dim = trainxn.shape[2]\n",
    "    means = np.zeros((dim, 1))\n",
    "    stds = np.zeros((dim, 1))\n",
    "    for n in range(dim): # Apply normalization to the data that is already in Pytorch format\n",
    "        means[n, ] = np.mean(trainxn[:, :, n, :, :])\n",
    "        stds[n, ] = np.std(trainxn[:, :, n, :, :])\n",
    "        trainxn[:, :, n, :, :] = (trainxn[:, :, n, :, :] - means[n, ]) / (stds[n, ])\n",
    "    return trainxn, means, stds\n",
    "\n",
    "\n",
    "def applynormalize(testx, means, stds):\n",
    "    \"\"\"Apply normalization based on previous calculated means and stds\"\"\"\n",
    "    testxn = testx.copy()\n",
    "    for n in range(testx.shape[2]):\n",
    "        testxn[:, :, n, :, :] = (testxn[:, :, n, :, :] - means[n, ]) / (stds[n, ])\n",
    "    return testxn"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-cXqMxfMB4G"
   },
   "source": [
    "Bands selected by our **Inter-Band Redundancy Analysis -- Greedy Spectral Selection (IBRA-GSS) method**: \n",
    "[11, 25, 34, 39, 67]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3exX4DQlJCnm",
    "outputId": "3489fcb0-0daf-4cde-82e0-6f2c33f7d9ee",
    "ExecuteTime": {
     "end_time": "2024-08-06T08:22:22.540883Z",
     "start_time": "2024-08-06T08:15:13.584733Z"
    }
   },
   "source": [
    "indexes = [11, 25, 34, 39, 67]  # For SA: [37, 60, 82, 92, 175]\n",
    "train_X_selected = select(trainX, indexes)\n",
    "val_X_selected = select(valX, indexes)\n",
    "print(train_X_selected.shape)\n",
    "# Normalize using the training set\n",
    "train_X_selected, means, stds = normalize(train_X_selected)\n",
    "# Apply the same normalization to the validation set\n",
    "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
    "\n",
    "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
    "model = CNNTrainer()\n",
    "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
    "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
    "                        batch_size=128, epochs=50, filepath=\"temp_model\", printProcess=False)  # Set printProcess=True to see the training process \n",
    "\n",
    "# Validate\n",
    "model.loadModel(\"temp_model\")\n",
    "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
    "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
    "oa = correct_pred.sum() / len(correct_pred) * 100\n",
    "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
    "\n",
    "print(\"Accuracy = \" + str(oa))\n",
    "print(\"Precision = \" + str(prec))\n",
    "print(\"Recall = \" + str(rec))\n",
    "print(\"F1 = \" + str(f1))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5124, 1, 5, 5, 5)\n",
      "Training model.....\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[11], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m CNNTrainer()\n\u001B[0;32m     12\u001B[0m model\u001B[38;5;241m.\u001B[39mdefineModel(nbands\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, windowSize\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, train_y\u001B[38;5;241m=\u001B[39mtrainY)\n\u001B[1;32m---> 13\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainFold\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_X_selected\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrainY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mval_X_selected\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvaly\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m128\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfilepath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemp_model\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprintProcess\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Set printProcess=True to see the training process \u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Validate\u001B[39;00m\n\u001B[0;32m     17\u001B[0m model\u001B[38;5;241m.\u001B[39mloadModel(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtemp_model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[8], line 122\u001B[0m, in \u001B[0;36mCNNTrainer.trainFold\u001B[1;34m(self, trainx, trainy, batch_size, epochs, valx, valy, filepath, printProcess)\u001B[0m\n\u001B[0;32m    120\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mnetwork(trainxb)\n\u001B[0;32m    121\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mcriterion(outputs, trainyb)\n\u001B[1;32m--> 122\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39moptimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m    125\u001B[0m \u001B[38;5;66;03m# print statistics\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HSI-BandSelection-master\\lib\\site-packages\\torch\\_tensor.py:488\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    480\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    481\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    486\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    487\u001B[0m     )\n\u001B[1;32m--> 488\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\HSI-BandSelection-master\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    192\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    194\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 197\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    198\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    199\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T06:49:12.487288Z",
     "start_time": "2024-08-06T06:49:12.459362Z"
    },
    "id": "jjyBQO5Qw7Ji"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Reset weights\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mnetwork\u001B[38;5;241m.\u001B[39mapply(weight_reset)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Reset weights\n",
    "model.model.network.apply(weight_reset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "To2DP0dTxZah"
   },
   "source": [
    "Bands selected by the Fast Neighborhood Grouping Method for Hyperspectral Band Selection (FNGBS) method (https://github.com/qianngli/FNGBS): [28, 70, 92, 107, 129]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T06:49:32.187867Z",
     "start_time": "2024-08-06T06:49:32.148096Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xVWT4Up1xyhi",
    "outputId": "91094151-31e5-4b19-8c01-527b859b3593"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[17], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m indexes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m28\u001B[39m, \u001B[38;5;241m70\u001B[39m, \u001B[38;5;241m92\u001B[39m, \u001B[38;5;241m107\u001B[39m, \u001B[38;5;241m129\u001B[39m]  \u001B[38;5;66;03m# For SA: [16, 31, 113, 132, 175]\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m train_X_selected \u001B[38;5;241m=\u001B[39m select(\u001B[43mtrainX\u001B[49m, indexes)\n\u001B[0;32m      3\u001B[0m val_X_selected \u001B[38;5;241m=\u001B[39m select(valX, indexes)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Normalize using the training set\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'trainX' is not defined"
     ]
    }
   ],
   "source": [
    "indexes = [28, 70, 92, 107, 129]  # For SA: [16, 31, 113, 132, 175]\n",
    "train_X_selected = select(trainX, indexes)\n",
    "val_X_selected = select(valX, indexes)\n",
    "\n",
    "# Normalize using the training set\n",
    "train_X_selected, means, stds = normalize(train_X_selected)\n",
    "# Apply the same normalization to the validation set\n",
    "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
    "\n",
    "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
    "model = CNNTrainer()\n",
    "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
    "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
    "                        batch_size=128, epochs=50, filepath=\"temp_model\")\n",
    "\n",
    "# Validate\n",
    "model.loadModel(\"temp_model\")\n",
    "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
    "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
    "oa = correct_pred.sum() / len(correct_pred) * 100\n",
    "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
    "\n",
    "print(\"Accuracy = \" + str(oa))\n",
    "print(\"Precision = \" + str(prec))\n",
    "print(\"Recall = \" + str(rec))\n",
    "print(\"F1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T06:49:36.225463Z",
     "start_time": "2024-08-06T06:49:36.200531Z"
    },
    "id": "yZZ4SukKz31o"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Reset weights\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mnetwork\u001B[38;5;241m.\u001B[39mapply(weight_reset)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Reset weights\n",
    "model.model.network.apply(weight_reset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZXJ1e2kzHqI"
   },
   "source": [
    "Bands selected by the Similarity-Based Ranking (SRSSIM) method (https://ieeexplore.ieee.org/document/9324974): [28, 52, 91, 104, 121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T06:49:41.827159Z",
     "start_time": "2024-08-06T06:49:41.781705Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MgeqBSsvzd_4",
    "outputId": "b1d29a51-be77-408f-ef42-3f2a76427301"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m indexes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m28\u001B[39m, \u001B[38;5;241m52\u001B[39m, \u001B[38;5;241m91\u001B[39m, \u001B[38;5;241m104\u001B[39m, \u001B[38;5;241m121\u001B[39m]  \u001B[38;5;66;03m# For SA: [5, 47, 61, 81, 201]\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m train_X_selected \u001B[38;5;241m=\u001B[39m select(\u001B[43mtrainX\u001B[49m, indexes)\n\u001B[0;32m      3\u001B[0m val_X_selected \u001B[38;5;241m=\u001B[39m select(valX, indexes)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Normalize using the training set\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'trainX' is not defined"
     ]
    }
   ],
   "source": [
    "indexes = [28, 52, 91, 104, 121]  # For SA: [5, 47, 61, 81, 201]\n",
    "train_X_selected = select(trainX, indexes)\n",
    "val_X_selected = select(valX, indexes)\n",
    "\n",
    "# Normalize using the training set\n",
    "train_X_selected, means, stds = normalize(train_X_selected)\n",
    "# Apply the same normalization to the validation set\n",
    "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
    "\n",
    "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
    "model = CNNTrainer()\n",
    "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
    "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
    "                        batch_size=128, epochs=50, filepath=\"temp_model\")\n",
    "\n",
    "# Validate\n",
    "model.loadModel(\"temp_model\")\n",
    "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
    "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
    "oa = correct_pred.sum() / len(correct_pred) * 100\n",
    "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
    "\n",
    "print(\"Accuracy = \" + str(oa))\n",
    "print(\"Precision = \" + str(prec))\n",
    "print(\"Recall = \" + str(rec))\n",
    "print(\"F1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T06:49:46.677931Z",
     "start_time": "2024-08-06T06:49:46.646019Z"
    },
    "id": "5HoxzQdLz4zY"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Reset weights\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mnetwork\u001B[38;5;241m.\u001B[39mapply(weight_reset)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Reset weights\n",
    "model.model.network.apply(weight_reset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykOuh7qgzkjP"
   },
   "source": [
    "Bands selected by the Optimal Clustering Framework (OCF) method (https://ieeexplore.ieee.org/document/8356741/): [16, 28, 50, 67, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T06:49:54.628683Z",
     "start_time": "2024-08-06T06:49:54.583768Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3a8XJuzzzEY",
    "outputId": "cd1e43cb-b2ad-43c6-d2cf-347362f49974"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m indexes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m16\u001B[39m, \u001B[38;5;241m28\u001B[39m, \u001B[38;5;241m50\u001B[39m, \u001B[38;5;241m67\u001B[39m, \u001B[38;5;241m90\u001B[39m]  \u001B[38;5;66;03m# For SA: [34, 45, 58, 93, 120]\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m train_X_selected \u001B[38;5;241m=\u001B[39m select(\u001B[43mtrainX\u001B[49m, indexes)\n\u001B[0;32m      3\u001B[0m val_X_selected \u001B[38;5;241m=\u001B[39m select(valX, indexes)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Normalize using the training set\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'trainX' is not defined"
     ]
    }
   ],
   "source": [
    "indexes = [16, 28, 50, 67, 90]  # For SA: [34, 45, 58, 93, 120]\n",
    "train_X_selected = select(trainX, indexes)\n",
    "val_X_selected = select(valX, indexes)\n",
    "\n",
    "# Normalize using the training set\n",
    "train_X_selected, means, stds = normalize(train_X_selected)\n",
    "# Apply the same normalization to the validation set\n",
    "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
    "\n",
    "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
    "model = CNNTrainer()\n",
    "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
    "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
    "                        batch_size=128, epochs=50, filepath=\"temp_model\")\n",
    "\n",
    "# Validate\n",
    "model.loadModel(\"temp_model\")\n",
    "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
    "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
    "oa = correct_pred.sum() / len(correct_pred) * 100\n",
    "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
    "\n",
    "print(\"Accuracy = \" + str(oa))\n",
    "print(\"Precision = \" + str(prec))\n",
    "print(\"Recall = \" + str(rec))\n",
    "print(\"F1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T06:50:03.957628Z",
     "start_time": "2024-08-06T06:50:03.926714Z"
    },
    "id": "GG63KQSCz5nG"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Reset weights\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mnetwork\u001B[38;5;241m.\u001B[39mapply(weight_reset)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Reset weights\n",
    "model.model.network.apply(weight_reset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdkXpb4u0IPx"
   },
   "source": [
    "Bands selected by the Histogram Assisted Genetic Algorithm for Reduction in Dimensionality (HAGRID) method (https://www.researchgate.net/publication/334216691_Using_a_genetic_algorithm_with_histogram-based_feature_selection_in_hyperspectral_image_classification): [17, 31, 55, 75, 119]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T06:50:07.144259Z",
     "start_time": "2024-08-06T06:50:07.105151Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "46vkNNJI0jeB",
    "outputId": "83eb9da7-692d-40ed-bc92-08c5467aa501"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainX' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[24], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m indexes \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m17\u001B[39m, \u001B[38;5;241m31\u001B[39m, \u001B[38;5;241m55\u001B[39m, \u001B[38;5;241m75\u001B[39m, \u001B[38;5;241m119\u001B[39m]  \u001B[38;5;66;03m# For SA: [13, 20, 31, 44, 84]\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m train_X_selected \u001B[38;5;241m=\u001B[39m select(\u001B[43mtrainX\u001B[49m, indexes)\n\u001B[0;32m      3\u001B[0m val_X_selected \u001B[38;5;241m=\u001B[39m select(valX, indexes)\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Normalize using the training set\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'trainX' is not defined"
     ]
    }
   ],
   "source": [
    "indexes = [17, 31, 55, 75, 119]  # For SA: [13, 20, 31, 44, 84]\n",
    "train_X_selected = select(trainX, indexes)\n",
    "val_X_selected = select(valX, indexes)\n",
    "\n",
    "# Normalize using the training set\n",
    "train_X_selected, means, stds = normalize(train_X_selected)\n",
    "# Apply the same normalization to the validation set\n",
    "val_X_selected = applynormalize(val_X_selected, means, stds)\n",
    "\n",
    "# Initialize model and train (USE GPU!: Runtime -> Change runtime type)\n",
    "model = CNNTrainer()\n",
    "model.defineModel(nbands=5, windowSize=5, train_y=trainY)\n",
    "model.trainFold(trainx=train_X_selected, trainy=trainY, valx=val_X_selected, valy=valY, \n",
    "                        batch_size=128, epochs=50, filepath=\"temp_model\")\n",
    "\n",
    "# Validate\n",
    "model.loadModel(\"temp_model\")\n",
    "ytest, ypred = model.evaluateFold(valx=val_X_selected, valy=valY, batch_size=128)\n",
    "correct_pred = (np.array(ypred) == ytest).astype(float)\n",
    "oa = correct_pred.sum() / len(correct_pred) * 100\n",
    "prec, rec, f1, support = precision_recall_fscore_support(ytest, ypred, average='macro')\n",
    "\n",
    "print(\"Accuracy = \" + str(oa))\n",
    "print(\"Precision = \" + str(prec))\n",
    "print(\"Recall = \" + str(rec))\n",
    "print(\"F1 = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM2+1uh4qaJcHK+hQ7TZMpg",
   "include_colab_link": true,
   "name": "IP-SA-Demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
